{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(r\"/Users/shwetabhat/Desktop/ATML/ProjectTask/DataPrepAndFeatureExt2/PreprocessedNoStopWords.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>guten_genre</th>\n",
       "      <th>clean_data</th>\n",
       "      <th>Book_Name0</th>\n",
       "      <th>Book_Name1</th>\n",
       "      <th>Book_Name2</th>\n",
       "      <th>Book_Name3</th>\n",
       "      <th>Book_Name4</th>\n",
       "      <th>Book_Name5</th>\n",
       "      <th>Book_Name6</th>\n",
       "      <th>Book_Name7</th>\n",
       "      <th>...</th>\n",
       "      <th>Author_Name496</th>\n",
       "      <th>Author_Name497</th>\n",
       "      <th>Author_Name498</th>\n",
       "      <th>Author_Name499</th>\n",
       "      <th>Author_Name500</th>\n",
       "      <th>Author_Name501</th>\n",
       "      <th>Author_Name502</th>\n",
       "      <th>Author_Name503</th>\n",
       "      <th>Author_Name504</th>\n",
       "      <th>Author_Name505</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Literary</td>\n",
       "      <td>love seeketh self please bind another delight...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Literary</td>\n",
       "      <td>james dear james upon time dedicated tale dee...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Literary</td>\n",
       "      <td>tokyo tijuana gabriele departing america stev...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 1500 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  guten_genre                                         clean_data  Book_Name0  \\\n",
       "0    Literary   love seeketh self please bind another delight...         0.0   \n",
       "1    Literary   james dear james upon time dedicated tale dee...         0.0   \n",
       "2    Literary   tokyo tijuana gabriele departing america stev...         0.0   \n",
       "\n",
       "   Book_Name1  Book_Name2  Book_Name3  Book_Name4  Book_Name5  Book_Name6  \\\n",
       "0         0.0         0.0         0.0         0.0         0.0         0.0   \n",
       "1         0.0         0.0         0.0         0.0         0.0         0.0   \n",
       "2         0.0         0.0         0.0         0.0         0.0         0.0   \n",
       "\n",
       "   Book_Name7  ...  Author_Name496  Author_Name497  Author_Name498  \\\n",
       "0         0.0  ...             0.0             0.0             0.0   \n",
       "1         0.0  ...             0.0             0.0             0.0   \n",
       "2         0.0  ...             0.0             0.0             0.0   \n",
       "\n",
       "   Author_Name499  Author_Name500  Author_Name501  Author_Name502  \\\n",
       "0             0.0             0.0             0.0             0.0   \n",
       "1             0.0             0.0             0.0             0.0   \n",
       "2             0.0             0.0             0.0             0.0   \n",
       "\n",
       "   Author_Name503  Author_Name504  Author_Name505  \n",
       "0             0.0             0.0             0.0  \n",
       "1             0.0             0.0             0.0  \n",
       "2             0.0             0.0             0.0  \n",
       "\n",
       "[3 rows x 1500 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del data['Content']\n",
    "del data['id']\n",
    "data.head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc1 = OneHotEncoder(handle_unknown='ignore')\n",
    "enc_df1 = pd.DataFrame(enc1.fit_transform(data[['Book_Name']]).toarray())\n",
    " # merge with main df bridge_df on key values\n",
    "del data['Book_Name']\n",
    "col_name=[]\n",
    "for i in range(enc_df1.columns.start,enc_df1.columns.stop,enc_df1.columns.step):\n",
    "    x=\"Book_Name\"+(str(i))\n",
    "    col_name.append(x)\n",
    "enc_df1.columns=col_name\n",
    "data = data.join(enc_df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc1 = OneHotEncoder(handle_unknown='ignore')\n",
    "enc_df1 = pd.DataFrame(enc1.fit_transform(data[['Author_Name']]).toarray())\n",
    " # merge with main df bridge_df on key values\n",
    "del data['Author_Name']\n",
    "col_name=[]\n",
    "for i in range(enc_df1.columns.start,enc_df1.columns.stop,enc_df1.columns.step):\n",
    "    x=\"Author_Name\"+(str(i))\n",
    "    col_name.append(x)\n",
    "enc_df1.columns=col_name\n",
    "data = data.join(enc_df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean_data</th>\n",
       "      <th>Book_Name0</th>\n",
       "      <th>Book_Name1</th>\n",
       "      <th>Book_Name2</th>\n",
       "      <th>Book_Name3</th>\n",
       "      <th>Book_Name4</th>\n",
       "      <th>Book_Name5</th>\n",
       "      <th>Book_Name6</th>\n",
       "      <th>Book_Name7</th>\n",
       "      <th>Book_Name8</th>\n",
       "      <th>...</th>\n",
       "      <th>Author_Name496</th>\n",
       "      <th>Author_Name497</th>\n",
       "      <th>Author_Name498</th>\n",
       "      <th>Author_Name499</th>\n",
       "      <th>Author_Name500</th>\n",
       "      <th>Author_Name501</th>\n",
       "      <th>Author_Name502</th>\n",
       "      <th>Author_Name503</th>\n",
       "      <th>Author_Name504</th>\n",
       "      <th>Author_Name505</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>love seeketh self please bind another delight...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>james dear james upon time dedicated tale dee...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>tokyo tijuana gabriele departing america stev...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>looking back many month struggle foreboding w...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>page last page frontispiece stretched upon ow...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>989</td>\n",
       "      <td>book pleased bestow especial affection dedica...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>990</td>\n",
       "      <td>still evening dust hung golden moment slowly ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>991</td>\n",
       "      <td>reginald philip graham thursford baron traver...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>992</td>\n",
       "      <td>bettina mowbray walking deck ocean steamer bo...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>993</td>\n",
       "      <td>miss heth page vivian paint upon window dwelt...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>994 rows × 1499 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            clean_data  Book_Name0  \\\n",
       "0     love seeketh self please bind another delight...         0.0   \n",
       "1     james dear james upon time dedicated tale dee...         0.0   \n",
       "2     tokyo tijuana gabriele departing america stev...         0.0   \n",
       "3     looking back many month struggle foreboding w...         0.0   \n",
       "4     page last page frontispiece stretched upon ow...         0.0   \n",
       "..                                                 ...         ...   \n",
       "989   book pleased bestow especial affection dedica...         0.0   \n",
       "990   still evening dust hung golden moment slowly ...         0.0   \n",
       "991   reginald philip graham thursford baron traver...         0.0   \n",
       "992   bettina mowbray walking deck ocean steamer bo...         0.0   \n",
       "993   miss heth page vivian paint upon window dwelt...         0.0   \n",
       "\n",
       "     Book_Name1  Book_Name2  Book_Name3  Book_Name4  Book_Name5  Book_Name6  \\\n",
       "0           0.0         0.0         0.0         0.0         0.0         0.0   \n",
       "1           0.0         0.0         0.0         0.0         0.0         0.0   \n",
       "2           0.0         0.0         0.0         0.0         0.0         0.0   \n",
       "3           0.0         0.0         0.0         0.0         0.0         0.0   \n",
       "4           0.0         0.0         0.0         0.0         0.0         0.0   \n",
       "..          ...         ...         ...         ...         ...         ...   \n",
       "989         0.0         0.0         0.0         0.0         0.0         0.0   \n",
       "990         0.0         0.0         0.0         0.0         0.0         0.0   \n",
       "991         0.0         0.0         0.0         0.0         0.0         0.0   \n",
       "992         0.0         0.0         0.0         0.0         0.0         0.0   \n",
       "993         0.0         0.0         0.0         0.0         0.0         0.0   \n",
       "\n",
       "     Book_Name7  Book_Name8  ...  Author_Name496  Author_Name497  \\\n",
       "0           0.0         0.0  ...             0.0             0.0   \n",
       "1           0.0         0.0  ...             0.0             0.0   \n",
       "2           0.0         0.0  ...             0.0             0.0   \n",
       "3           0.0         0.0  ...             0.0             0.0   \n",
       "4           0.0         0.0  ...             0.0             0.0   \n",
       "..          ...         ...  ...             ...             ...   \n",
       "989         0.0         0.0  ...             0.0             0.0   \n",
       "990         0.0         0.0  ...             0.0             0.0   \n",
       "991         0.0         0.0  ...             0.0             0.0   \n",
       "992         0.0         0.0  ...             0.0             0.0   \n",
       "993         0.0         0.0  ...             0.0             0.0   \n",
       "\n",
       "     Author_Name498  Author_Name499  Author_Name500  Author_Name501  \\\n",
       "0               0.0             0.0             0.0             0.0   \n",
       "1               0.0             0.0             0.0             0.0   \n",
       "2               0.0             0.0             0.0             0.0   \n",
       "3               0.0             0.0             0.0             0.0   \n",
       "4               0.0             0.0             0.0             0.0   \n",
       "..              ...             ...             ...             ...   \n",
       "989             0.0             0.0             0.0             0.0   \n",
       "990             0.0             0.0             0.0             0.0   \n",
       "991             0.0             0.0             0.0             0.0   \n",
       "992             0.0             0.0             0.0             0.0   \n",
       "993             0.0             0.0             0.0             0.0   \n",
       "\n",
       "     Author_Name502  Author_Name503  Author_Name504  Author_Name505  \n",
       "0               0.0             0.0             0.0             0.0  \n",
       "1               0.0             0.0             0.0             0.0  \n",
       "2               0.0             0.0             0.0             0.0  \n",
       "3               0.0             0.0             0.0             0.0  \n",
       "4               0.0             0.0             0.0             0.0  \n",
       "..              ...             ...             ...             ...  \n",
       "989             0.0             0.0             0.0             0.0  \n",
       "990             0.0             0.0             0.0             0.0  \n",
       "991             0.0             0.0             0.0             0.0  \n",
       "992             0.0             0.0             0.0             0.0  \n",
       "993             0.0             0.0             0.0             0.0  \n",
       "\n",
       "[994 rows x 1499 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels=data['guten_genre']\n",
    "cols=list(data.columns)\n",
    "cols=cols[1:]\n",
    "data=data[cols]\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test,y_train,y_test = train_test_split(data,labels, test_size=0.3, random_state=42, stratify = labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean_data</th>\n",
       "      <th>Book_Name0</th>\n",
       "      <th>Book_Name1</th>\n",
       "      <th>Book_Name2</th>\n",
       "      <th>Book_Name3</th>\n",
       "      <th>Book_Name4</th>\n",
       "      <th>Book_Name5</th>\n",
       "      <th>Book_Name6</th>\n",
       "      <th>Book_Name7</th>\n",
       "      <th>Book_Name8</th>\n",
       "      <th>...</th>\n",
       "      <th>Author_Name496</th>\n",
       "      <th>Author_Name497</th>\n",
       "      <th>Author_Name498</th>\n",
       "      <th>Author_Name499</th>\n",
       "      <th>Author_Name500</th>\n",
       "      <th>Author_Name501</th>\n",
       "      <th>Author_Name502</th>\n",
       "      <th>Author_Name503</th>\n",
       "      <th>Author_Name504</th>\n",
       "      <th>Author_Name505</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>early june year taking walk hampstead heath f...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>376</td>\n",
       "      <td>solitary room midnight single candle lighted ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>sunrise first april appeared suddenly manco c...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 1499 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            clean_data  Book_Name0  \\\n",
       "330   early june year taking walk hampstead heath f...         0.0   \n",
       "376   solitary room midnight single candle lighted ...         0.0   \n",
       "560   sunrise first april appeared suddenly manco c...         0.0   \n",
       "\n",
       "     Book_Name1  Book_Name2  Book_Name3  Book_Name4  Book_Name5  Book_Name6  \\\n",
       "330         0.0         0.0         0.0         0.0         0.0         0.0   \n",
       "376         0.0         0.0         0.0         0.0         0.0         0.0   \n",
       "560         0.0         0.0         0.0         0.0         0.0         0.0   \n",
       "\n",
       "     Book_Name7  Book_Name8  ...  Author_Name496  Author_Name497  \\\n",
       "330         0.0         0.0  ...             0.0             0.0   \n",
       "376         0.0         0.0  ...             0.0             0.0   \n",
       "560         0.0         0.0  ...             0.0             0.0   \n",
       "\n",
       "     Author_Name498  Author_Name499  Author_Name500  Author_Name501  \\\n",
       "330             0.0             0.0             0.0             0.0   \n",
       "376             0.0             0.0             0.0             0.0   \n",
       "560             0.0             0.0             0.0             0.0   \n",
       "\n",
       "     Author_Name502  Author_Name503  Author_Name504  Author_Name505  \n",
       "330             0.0             0.0             0.0             0.0  \n",
       "376             0.0             0.0             0.0             0.0  \n",
       "560             0.0             0.0             0.0             0.0  \n",
       "\n",
       "[3 rows x 1499 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(sublinear_tf=True)\n",
    "train = vectorizer.fit_transform(X_train['clean_data'])\n",
    "train=pd.DataFrame(train.todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = vectorizer.transform(X_test['clean_data'])\n",
    "test=pd.DataFrame(test.todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(695, 114311)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>Author_Name496</th>\n",
       "      <th>Author_Name497</th>\n",
       "      <th>Author_Name498</th>\n",
       "      <th>Author_Name499</th>\n",
       "      <th>Author_Name500</th>\n",
       "      <th>Author_Name501</th>\n",
       "      <th>Author_Name502</th>\n",
       "      <th>Author_Name503</th>\n",
       "      <th>Author_Name504</th>\n",
       "      <th>Author_Name505</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 115810 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     0    1    2    3    4    5    6    7    8    9  ...  Author_Name496  \\\n",
       "0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...             0.0   \n",
       "1  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...             0.0   \n",
       "2  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...             0.0   \n",
       "3  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...             0.0   \n",
       "4  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...             0.0   \n",
       "\n",
       "   Author_Name497  Author_Name498  Author_Name499  Author_Name500  \\\n",
       "0             0.0             0.0             0.0             0.0   \n",
       "1             0.0             0.0             0.0             0.0   \n",
       "2             0.0             0.0             0.0             0.0   \n",
       "3             0.0             0.0             0.0             0.0   \n",
       "4             0.0             0.0             0.0             0.0   \n",
       "\n",
       "   Author_Name501  Author_Name502  Author_Name503  Author_Name504  \\\n",
       "0             0.0             0.0             0.0             0.0   \n",
       "1             0.0             0.0             0.0             0.0   \n",
       "2             0.0             0.0             0.0             0.0   \n",
       "3             0.0             0.0             0.0             0.0   \n",
       "4             0.0             0.0             0.0             0.0   \n",
       "\n",
       "   Author_Name505  \n",
       "0             0.0  \n",
       "1             0.0  \n",
       "2             0.0  \n",
       "3             0.0  \n",
       "4             0.0  \n",
       "\n",
       "[5 rows x 115810 columns]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train=X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.reset_index(drop=True, inplace=True)    \n",
    "X_train.reset_index(drop=True, inplace=True)\n",
    "\n",
    "X_train=pd.concat([train,X_train],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.reset_index(drop=True, inplace=True)    \n",
    "X_test.reset_index(drop=True, inplace=True)\n",
    "\n",
    "X_test=pd.concat([test,X_test],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=X_train\n",
    "test=X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "114311"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aachen',\n",
       " 'aaem',\n",
       " 'aair',\n",
       " 'aalsund',\n",
       " 'aamand',\n",
       " 'aamy',\n",
       " 'aamænd',\n",
       " 'aaoow',\n",
       " 'aaraaf',\n",
       " 'aargau',\n",
       " 'aarhus',\n",
       " 'aaron',\n",
       " 'aase',\n",
       " 'aback',\n",
       " 'abaddon',\n",
       " 'abaft',\n",
       " 'abaho',\n",
       " 'abaht',\n",
       " 'abalone',\n",
       " 'abana',\n",
       " 'abandon',\n",
       " 'abandoned',\n",
       " 'abandonedly',\n",
       " 'abandoning',\n",
       " 'abandonment',\n",
       " 'abanijo',\n",
       " 'abart',\n",
       " 'abase',\n",
       " 'abased',\n",
       " 'abasement',\n",
       " 'abash',\n",
       " 'abashed',\n",
       " 'abashes',\n",
       " 'abashing',\n",
       " 'abashingly',\n",
       " 'abashment',\n",
       " 'abasing',\n",
       " 'abate',\n",
       " 'abated',\n",
       " 'abatement',\n",
       " 'abates',\n",
       " 'abating',\n",
       " 'abatis',\n",
       " 'abatross',\n",
       " 'abattez',\n",
       " 'abattis',\n",
       " 'abattoir',\n",
       " 'abattre',\n",
       " 'abawt',\n",
       " 'abazzia',\n",
       " 'abbaside',\n",
       " 'abbassieh',\n",
       " 'abbate',\n",
       " 'abbati',\n",
       " 'abbatial',\n",
       " 'abbaye',\n",
       " 'abbe',\n",
       " 'abbencombe',\n",
       " 'abberley',\n",
       " 'abbess',\n",
       " 'abbetting',\n",
       " 'abbeville',\n",
       " 'abbey',\n",
       " 'abbeyland',\n",
       " 'abbia',\n",
       " 'abbie',\n",
       " 'abbot',\n",
       " 'abbotsbury',\n",
       " 'abbotsford',\n",
       " 'abbott',\n",
       " 'abbotts',\n",
       " 'abbozzatore',\n",
       " 'abbrazza',\n",
       " 'abbreviate',\n",
       " 'abbreviated',\n",
       " 'abbreviating',\n",
       " 'abbreviation',\n",
       " 'abbreviative',\n",
       " 'abbrucia',\n",
       " 'abby',\n",
       " 'abbé',\n",
       " 'abbés',\n",
       " 'abc',\n",
       " 'abcdef',\n",
       " 'abdallah',\n",
       " 'abdera',\n",
       " 'abderite',\n",
       " 'abderitish',\n",
       " 'abdi',\n",
       " 'abdicate',\n",
       " 'abdicated',\n",
       " 'abdicates',\n",
       " 'abdicating',\n",
       " 'abdication',\n",
       " 'abdiel',\n",
       " 'abdiels',\n",
       " 'abdingdon',\n",
       " 'abdomen',\n",
       " 'abdominal',\n",
       " 'abduct',\n",
       " 'abducted',\n",
       " 'abductin',\n",
       " 'abducting',\n",
       " 'abduction',\n",
       " 'abductor',\n",
       " 'abducts',\n",
       " 'abdul',\n",
       " 'abdullah',\n",
       " 'abdur',\n",
       " 'abdy',\n",
       " 'abea',\n",
       " 'abeam',\n",
       " 'abear',\n",
       " 'abeating',\n",
       " 'abecedarian',\n",
       " 'abed',\n",
       " 'abednego',\n",
       " 'abeen',\n",
       " 'abeing',\n",
       " 'abel',\n",
       " 'abelard',\n",
       " 'abelwhite',\n",
       " 'abend',\n",
       " 'abendbrod',\n",
       " 'abener',\n",
       " 'aber',\n",
       " 'aberconway',\n",
       " 'abercorn',\n",
       " 'abercorns',\n",
       " 'abercroft',\n",
       " 'abercrombie',\n",
       " 'abercromby',\n",
       " 'aberdeen',\n",
       " 'aberdeenshire',\n",
       " 'aberford',\n",
       " 'aberg',\n",
       " 'abergavenny',\n",
       " 'aberkilvie',\n",
       " 'abermarle',\n",
       " 'abernethy',\n",
       " 'abernetty',\n",
       " 'aberrant',\n",
       " 'aberration',\n",
       " 'abershaw',\n",
       " 'abertawe',\n",
       " 'aberystwyth',\n",
       " 'abet',\n",
       " 'abetment',\n",
       " 'abets',\n",
       " 'abetted',\n",
       " 'abetter',\n",
       " 'abetting',\n",
       " 'abettor',\n",
       " 'abey',\n",
       " 'abeyance',\n",
       " 'abgeordneten',\n",
       " 'abhor',\n",
       " 'abhorence',\n",
       " 'abhorent',\n",
       " 'abhorre',\n",
       " 'abhorred',\n",
       " 'abhorrence',\n",
       " 'abhorrent',\n",
       " 'abhorrently',\n",
       " 'abhorrer',\n",
       " 'abhorring',\n",
       " 'abhorringly',\n",
       " 'abhors',\n",
       " 'abiathar',\n",
       " 'abide',\n",
       " 'abided',\n",
       " 'abides',\n",
       " 'abidest',\n",
       " 'abideth',\n",
       " 'abiding',\n",
       " 'abidingly',\n",
       " 'abidingness',\n",
       " 'abigail',\n",
       " 'abigails',\n",
       " 'abijah',\n",
       " 'abilene',\n",
       " 'ability',\n",
       " 'abine',\n",
       " 'abingdon',\n",
       " 'abipones',\n",
       " 'abiram',\n",
       " 'abishag',\n",
       " 'abisheg',\n",
       " 'abit',\n",
       " 'abitao',\n",
       " 'abject',\n",
       " 'abjection',\n",
       " 'abjectly',\n",
       " 'abjectness',\n",
       " 'abjectus',\n",
       " 'abjuration',\n",
       " 'abjure',\n",
       " 'abjured',\n",
       " 'abjures',\n",
       " 'abjuring',\n",
       " 'ablated',\n",
       " 'ablative',\n",
       " 'ablaze',\n",
       " 'able',\n",
       " 'ableness',\n",
       " 'abler',\n",
       " 'ablest',\n",
       " 'ablinkin',\n",
       " 'abloo',\n",
       " 'abloom',\n",
       " 'ablow',\n",
       " 'ablowing',\n",
       " 'ablution',\n",
       " 'ablutionary',\n",
       " 'ably',\n",
       " 'abnegated',\n",
       " 'abnegating',\n",
       " 'abnegation',\n",
       " 'abner',\n",
       " 'abney',\n",
       " 'abnormal',\n",
       " 'abnormality',\n",
       " 'abnormally',\n",
       " 'abnormity',\n",
       " 'aboab',\n",
       " 'aboabs',\n",
       " 'aboard',\n",
       " 'aboardin',\n",
       " 'aboardship',\n",
       " 'abode',\n",
       " 'aboil',\n",
       " 'abolish',\n",
       " 'abolished',\n",
       " 'abolishing',\n",
       " 'abolishment',\n",
       " 'abolition',\n",
       " 'abolitionism',\n",
       " 'abolitionist',\n",
       " 'aboltraqu',\n",
       " 'abominable',\n",
       " 'abominably',\n",
       " 'abominate',\n",
       " 'abominated',\n",
       " 'abominates',\n",
       " 'abominating',\n",
       " 'abomination',\n",
       " 'abon',\n",
       " 'abonnement',\n",
       " 'aboo',\n",
       " 'aboon',\n",
       " 'aboord',\n",
       " 'aboot',\n",
       " 'aboove',\n",
       " 'abord',\n",
       " 'aboriginal',\n",
       " 'aboriginally',\n",
       " 'aboriginalness',\n",
       " 'aborigine',\n",
       " 'abort',\n",
       " 'aborted',\n",
       " 'aborting',\n",
       " 'abortion',\n",
       " 'abortioned',\n",
       " 'abortive',\n",
       " 'abortively',\n",
       " 'aboukir',\n",
       " 'abound',\n",
       " 'abounded',\n",
       " 'abounding',\n",
       " 'aboundingly',\n",
       " 'abounds',\n",
       " 'aboutepounding',\n",
       " 'abouts',\n",
       " 'above',\n",
       " 'aboveboard',\n",
       " 'abovementioned',\n",
       " 'abovestairs',\n",
       " 'abracadabra',\n",
       " 'abradbanya',\n",
       " 'abrade',\n",
       " 'abraded',\n",
       " 'abrading',\n",
       " 'abraham',\n",
       " 'abraised',\n",
       " 'abram',\n",
       " 'abrams',\n",
       " 'abran',\n",
       " 'abrased',\n",
       " 'abrasia',\n",
       " 'abrasion',\n",
       " 'abrasive',\n",
       " 'abrasively',\n",
       " 'abrazza',\n",
       " 'abread',\n",
       " 'abreast',\n",
       " 'abregès',\n",
       " 'abrem',\n",
       " 'abridge',\n",
       " 'abridged',\n",
       " 'abridgement',\n",
       " 'abridging',\n",
       " 'abridgment',\n",
       " 'abrim',\n",
       " 'abrivade',\n",
       " 'abroach',\n",
       " 'abroad',\n",
       " 'abrodiaitos',\n",
       " 'abrogate',\n",
       " 'abrogated',\n",
       " 'abrogating',\n",
       " 'abrogation',\n",
       " 'abrogator',\n",
       " 'abrupt',\n",
       " 'abrupted',\n",
       " 'abruptedly',\n",
       " 'abrupter',\n",
       " 'abruptly',\n",
       " 'abruptness',\n",
       " 'abruzzi',\n",
       " 'absalom',\n",
       " 'absawd',\n",
       " 'abscess',\n",
       " 'abscies',\n",
       " 'abscissa',\n",
       " 'abscond',\n",
       " 'absconded',\n",
       " 'absconder',\n",
       " 'absconding',\n",
       " 'absence',\n",
       " 'absens',\n",
       " 'absense',\n",
       " 'absent',\n",
       " 'absented',\n",
       " 'absentee',\n",
       " 'absenteeism',\n",
       " 'absenting',\n",
       " 'absentis',\n",
       " 'absently',\n",
       " 'absentminded',\n",
       " 'absentmindedly',\n",
       " 'absentmindedness',\n",
       " 'absentness',\n",
       " 'absents',\n",
       " 'absinthe',\n",
       " 'absinthiated',\n",
       " 'absit',\n",
       " 'absogoshdarnlutely',\n",
       " 'absolom',\n",
       " 'absolum',\n",
       " 'absolument',\n",
       " 'absolumént',\n",
       " 'absolute',\n",
       " 'absolutely',\n",
       " 'absoluteness',\n",
       " 'absolution',\n",
       " 'absolutism',\n",
       " 'absolutist',\n",
       " 'absolve',\n",
       " 'absolved',\n",
       " 'absolver',\n",
       " 'absolves',\n",
       " 'absolving',\n",
       " 'absolvisti',\n",
       " 'absolvo',\n",
       " 'absolûment',\n",
       " 'absona',\n",
       " 'absorb',\n",
       " 'absorbed',\n",
       " 'absorbedly',\n",
       " 'absorbedness',\n",
       " 'absorbent',\n",
       " 'absorber',\n",
       " 'absorbing',\n",
       " 'absorbingly',\n",
       " 'absorbs',\n",
       " 'absorbtion',\n",
       " 'absorption',\n",
       " 'absorptive',\n",
       " 'absquatulated',\n",
       " 'abst',\n",
       " 'abstain',\n",
       " 'abstained',\n",
       " 'abstainer',\n",
       " 'abstaining',\n",
       " 'abstains',\n",
       " 'abstemious',\n",
       " 'abstemiously',\n",
       " 'abstemiousness',\n",
       " 'abstention',\n",
       " 'absterrebitur',\n",
       " 'abstinence',\n",
       " 'abstinent',\n",
       " 'abstract',\n",
       " 'abstracted',\n",
       " 'abstractedly',\n",
       " 'abstractedness',\n",
       " 'abstractest',\n",
       " 'abstracting',\n",
       " 'abstraction',\n",
       " 'abstractly',\n",
       " 'abstractness',\n",
       " 'abstractor',\n",
       " 'abstruse',\n",
       " 'abstrusely',\n",
       " 'abstruseness',\n",
       " 'abstruser',\n",
       " 'abstrusity',\n",
       " 'absurd',\n",
       " 'absurdedly',\n",
       " 'absurder',\n",
       " 'absurdes',\n",
       " 'absurdest',\n",
       " 'absurdity',\n",
       " 'absurdly',\n",
       " 'absurdum',\n",
       " 'abul',\n",
       " 'abulcasem',\n",
       " 'abundance',\n",
       " 'abundant',\n",
       " 'abundantly',\n",
       " 'abune',\n",
       " 'abuse',\n",
       " 'abused',\n",
       " 'abusedly',\n",
       " 'abuser',\n",
       " 'abusin',\n",
       " 'abusing',\n",
       " 'abusive',\n",
       " 'abusively',\n",
       " 'abusiveness',\n",
       " 'abut',\n",
       " 'abutment',\n",
       " 'abuts',\n",
       " 'abutted',\n",
       " 'abutting',\n",
       " 'abxut',\n",
       " 'abydos',\n",
       " 'abyedok',\n",
       " 'abysm',\n",
       " 'abysmal',\n",
       " 'abysmally',\n",
       " 'abyss',\n",
       " 'abyssinia',\n",
       " 'abyssinian',\n",
       " 'abyssus',\n",
       " 'abâyeh',\n",
       " 'acacia',\n",
       " 'academe',\n",
       " 'academia',\n",
       " 'academic',\n",
       " 'academical',\n",
       " 'academically',\n",
       " 'academician',\n",
       " 'academists',\n",
       " 'academy',\n",
       " 'acadia',\n",
       " 'acadian',\n",
       " 'académie',\n",
       " 'académique',\n",
       " 'acajou',\n",
       " 'acamarichtzin',\n",
       " 'acanthus',\n",
       " 'acaule',\n",
       " 'acause',\n",
       " 'acca',\n",
       " 'accademia',\n",
       " 'acccount',\n",
       " 'accede',\n",
       " 'acceded',\n",
       " 'accedence',\n",
       " 'accedes',\n",
       " 'acceding',\n",
       " 'accelerate',\n",
       " 'accelerated',\n",
       " 'accelerates',\n",
       " 'accelerating',\n",
       " 'acceleration',\n",
       " 'accelerator',\n",
       " 'accenna',\n",
       " 'accent',\n",
       " 'accented',\n",
       " 'accenting',\n",
       " 'accentless',\n",
       " 'accentuate',\n",
       " 'accentuated',\n",
       " 'accentuating',\n",
       " 'accentuation',\n",
       " 'accentuators',\n",
       " 'accep',\n",
       " 'accept',\n",
       " 'acceptability',\n",
       " 'acceptable',\n",
       " 'acceptably',\n",
       " 'acceptance',\n",
       " 'acceptation',\n",
       " 'accepted',\n",
       " 'accepting',\n",
       " 'acceptit',\n",
       " 'acceptor',\n",
       " 'accepts',\n",
       " 'access',\n",
       " 'accessary',\n",
       " 'accesse',\n",
       " 'accessibility',\n",
       " 'accessible',\n",
       " 'accessibles',\n",
       " 'accession',\n",
       " 'accessoirés',\n",
       " 'accessory',\n",
       " 'acciden',\n",
       " 'accidence',\n",
       " 'accident',\n",
       " 'accidental',\n",
       " 'accidentalism',\n",
       " 'accidentally',\n",
       " 'accidente',\n",
       " 'accidenti',\n",
       " 'accidently',\n",
       " 'accidunt',\n",
       " 'accipe',\n",
       " 'accipients',\n",
       " 'acclaim',\n",
       " 'acclaimed',\n",
       " 'acclaiming',\n",
       " 'acclamation',\n",
       " 'acclamazioni',\n",
       " 'acclimate',\n",
       " 'acclimated',\n",
       " 'acclimating',\n",
       " 'acclimatise',\n",
       " 'acclimatised',\n",
       " 'acclimatization',\n",
       " 'acclimatized',\n",
       " 'acclivity',\n",
       " 'accolade',\n",
       " 'accomidatin',\n",
       " 'accommodate',\n",
       " 'accommodated',\n",
       " 'accommodates',\n",
       " 'accommodatin',\n",
       " 'accommodating',\n",
       " 'accommodatingly',\n",
       " 'accommodation',\n",
       " 'accommodazion',\n",
       " 'accomodate',\n",
       " 'accomodation',\n",
       " 'accompanied',\n",
       " 'accompanies',\n",
       " 'accompaniment',\n",
       " 'accompanist',\n",
       " 'accompany',\n",
       " 'accompanyest',\n",
       " 'accompanying',\n",
       " 'accompanyist',\n",
       " 'accompli',\n",
       " 'accomplice',\n",
       " 'accomplish',\n",
       " 'accomplished',\n",
       " 'accomplishes',\n",
       " 'accomplishing',\n",
       " 'accomplishment',\n",
       " 'accompliè',\n",
       " 'accompts',\n",
       " 'accoont',\n",
       " 'accoramboni',\n",
       " 'accord',\n",
       " 'accordance',\n",
       " 'accordant',\n",
       " 'accorded',\n",
       " 'accorden',\n",
       " 'accordeon',\n",
       " 'accordeons',\n",
       " 'accordin',\n",
       " 'according',\n",
       " 'accordingly',\n",
       " 'accordings',\n",
       " 'accordion',\n",
       " 'accorto',\n",
       " 'accost',\n",
       " 'accosted',\n",
       " 'accoster',\n",
       " 'accosters',\n",
       " 'accosting',\n",
       " 'accosts',\n",
       " 'accouchement',\n",
       " 'accoucheur',\n",
       " 'accoucheuse',\n",
       " 'account',\n",
       " 'accountability',\n",
       " 'accountable',\n",
       " 'accountancy',\n",
       " 'accountant',\n",
       " 'accounted',\n",
       " 'accountin',\n",
       " 'accounting',\n",
       " 'accoutered',\n",
       " 'accouterment',\n",
       " 'accoutred',\n",
       " 'accoutrement',\n",
       " 'accredit',\n",
       " 'accredited',\n",
       " 'accrediting',\n",
       " 'accrete',\n",
       " 'accretion',\n",
       " 'accrington',\n",
       " 'accringtons',\n",
       " 'accross',\n",
       " 'accru',\n",
       " 'accrue',\n",
       " 'accrued',\n",
       " 'accrues',\n",
       " 'accruing',\n",
       " 'acct',\n",
       " 'accts',\n",
       " 'accumulate',\n",
       " 'accumulated',\n",
       " 'accumulates',\n",
       " 'accumulating',\n",
       " 'accumulation',\n",
       " 'accumulative',\n",
       " 'accumulatively',\n",
       " 'accumulator',\n",
       " 'accuracy',\n",
       " 'accurate',\n",
       " 'accurately',\n",
       " 'accurse',\n",
       " 'accursed',\n",
       " 'accursedly',\n",
       " 'accursing',\n",
       " 'accurst',\n",
       " 'accusation',\n",
       " 'accusative',\n",
       " 'accusatory',\n",
       " 'accuse',\n",
       " 'accused',\n",
       " 'accuser',\n",
       " 'accuses',\n",
       " 'accusin',\n",
       " 'accusing',\n",
       " 'accusingly',\n",
       " 'accusit',\n",
       " 'accustom',\n",
       " 'accustomance',\n",
       " 'accustomary',\n",
       " 'accustomed',\n",
       " 'accustoming',\n",
       " 'accustoms',\n",
       " 'accès',\n",
       " 'acde',\n",
       " 'ace',\n",
       " 'aceldama',\n",
       " 'acequia',\n",
       " 'acequias',\n",
       " 'acer',\n",
       " 'acerb',\n",
       " 'acerbating',\n",
       " 'acerbic',\n",
       " 'acerbity',\n",
       " 'acerca',\n",
       " 'acetic',\n",
       " 'acetous',\n",
       " 'acetylene',\n",
       " 'acevido',\n",
       " 'achaia',\n",
       " 'achaian',\n",
       " 'achaion',\n",
       " 'achan',\n",
       " 'achates',\n",
       " 'achatours',\n",
       " 'achcha',\n",
       " 'ache',\n",
       " 'ached',\n",
       " 'achelous',\n",
       " 'achen',\n",
       " 'achenbach',\n",
       " 'achensee',\n",
       " 'acheron',\n",
       " 'acherontic',\n",
       " 'achieve',\n",
       " 'achieved',\n",
       " 'achievement',\n",
       " 'achiever',\n",
       " 'achieves',\n",
       " 'achieving',\n",
       " 'achill',\n",
       " 'achille',\n",
       " 'achillean',\n",
       " 'achilles',\n",
       " 'achilli',\n",
       " 'achillis',\n",
       " 'achin',\n",
       " 'aching',\n",
       " 'achingly',\n",
       " 'achitophel',\n",
       " 'achmed',\n",
       " 'achmet',\n",
       " 'achoke',\n",
       " 'achon',\n",
       " 'achree',\n",
       " 'achromatic',\n",
       " 'achs',\n",
       " 'achtsett',\n",
       " 'achæans',\n",
       " 'acid',\n",
       " 'acidity',\n",
       " 'acidly',\n",
       " 'acidulated',\n",
       " 'acidulous',\n",
       " 'acies',\n",
       " 'acis',\n",
       " 'ackerfortis',\n",
       " 'ackerman',\n",
       " 'ackinson',\n",
       " 'ackisation',\n",
       " 'ackland',\n",
       " 'ackley',\n",
       " 'acknowledge',\n",
       " 'acknowledged',\n",
       " 'acknowledgement',\n",
       " 'acknowledges',\n",
       " 'acknowledging',\n",
       " 'acknowledgment',\n",
       " 'ackroyd',\n",
       " 'acks',\n",
       " 'ackshally',\n",
       " 'ackshellay',\n",
       " 'ackworth',\n",
       " 'acme',\n",
       " 'acmé',\n",
       " 'acold',\n",
       " 'acolyte',\n",
       " 'acolyth',\n",
       " 'acomin',\n",
       " 'aconcagua',\n",
       " 'aconite',\n",
       " 'aconitine',\n",
       " 'acorn',\n",
       " 'acos',\n",
       " 'acoustic',\n",
       " 'acoustically',\n",
       " 'acquaint',\n",
       " 'acquaintance',\n",
       " 'acquaintanceship',\n",
       " 'acquainted',\n",
       " 'acquainting',\n",
       " 'acquaints',\n",
       " 'acquant',\n",
       " 'acquapendente',\n",
       " 'acquasole',\n",
       " 'acquatic',\n",
       " 'acqui',\n",
       " 'acquiesce',\n",
       " 'acquiesced',\n",
       " 'acquiescence',\n",
       " 'acquiescent',\n",
       " 'acquiescently',\n",
       " 'acquiesces',\n",
       " 'acquiescing',\n",
       " 'acquiesence',\n",
       " 'acquiline',\n",
       " 'acquire',\n",
       " 'acquired',\n",
       " 'acquirement',\n",
       " 'acquirendi',\n",
       " 'acquirens',\n",
       " 'acquirer',\n",
       " 'acquires',\n",
       " 'acquirin',\n",
       " 'acquiring',\n",
       " 'acquisition',\n",
       " 'acquisitive',\n",
       " 'acquisitively',\n",
       " 'acquisitiveness',\n",
       " 'acquit',\n",
       " 'acquitania',\n",
       " 'acquits',\n",
       " 'acquittal',\n",
       " 'acquittance',\n",
       " 'acquitted',\n",
       " 'acquitting',\n",
       " 'acrass',\n",
       " 'acre',\n",
       " 'acreage',\n",
       " 'acrid',\n",
       " 'acridity',\n",
       " 'acridly',\n",
       " 'acridness',\n",
       " 'acrimonious',\n",
       " 'acrimoniously',\n",
       " 'acrimony',\n",
       " 'acritelli',\n",
       " 'acrobat',\n",
       " 'acrobatic',\n",
       " 'acrobatics',\n",
       " 'acrodato',\n",
       " 'acropolis',\n",
       " 'acrorst',\n",
       " 'across',\n",
       " 'acrost',\n",
       " 'acrostic',\n",
       " 'acshully',\n",
       " 'act',\n",
       " 'actaeon',\n",
       " 'acte',\n",
       " 'acted',\n",
       " 'actest',\n",
       " 'acti',\n",
       " 'actial',\n",
       " 'actially',\n",
       " 'actilly',\n",
       " 'actin',\n",
       " 'acting',\n",
       " 'actinia',\n",
       " 'actiniadae',\n",
       " 'action',\n",
       " 'actionable',\n",
       " 'actium',\n",
       " 'active',\n",
       " 'actively',\n",
       " 'activeness',\n",
       " 'activest',\n",
       " 'activist',\n",
       " 'activity',\n",
       " 'actly',\n",
       " 'acton',\n",
       " 'actons',\n",
       " 'actor',\n",
       " 'actress',\n",
       " 'actual',\n",
       " 'actuality',\n",
       " 'actualization',\n",
       " 'actualized',\n",
       " 'actually',\n",
       " 'actuary',\n",
       " 'actuate',\n",
       " 'actuated',\n",
       " 'actuates',\n",
       " 'actuating',\n",
       " 'actuation',\n",
       " 'actum',\n",
       " 'actæon',\n",
       " 'actæons',\n",
       " 'acuerdo',\n",
       " 'acuity',\n",
       " 'acumapicthzin',\n",
       " 'acumen',\n",
       " 'acuminated',\n",
       " 'acupuncture',\n",
       " 'acushla',\n",
       " 'acushnet',\n",
       " 'acuta',\n",
       " 'acute',\n",
       " 'acutely',\n",
       " 'acuteness',\n",
       " 'acuter',\n",
       " 'acutest',\n",
       " 'acutiloba',\n",
       " 'adad',\n",
       " 'adage',\n",
       " 'adaged',\n",
       " 'adagio',\n",
       " 'adagioeyour',\n",
       " 'adagiooll',\n",
       " 'adah',\n",
       " 'adain',\n",
       " 'adainst',\n",
       " 'adair',\n",
       " 'adairs',\n",
       " 'adairsville',\n",
       " 'adalbert',\n",
       " 'adalina',\n",
       " 'adaline',\n",
       " 'adam',\n",
       " 'adamant',\n",
       " 'adamantine',\n",
       " 'adamantius',\n",
       " 'adamantly',\n",
       " 'adamantyne',\n",
       " 'adamello',\n",
       " 'adamic',\n",
       " 'adamite',\n",
       " 'adamitish',\n",
       " 'adams',\n",
       " 'adana',\n",
       " 'adantino',\n",
       " 'adapt',\n",
       " 'adaptability',\n",
       " 'adaptable',\n",
       " 'adaptation',\n",
       " 'adapted',\n",
       " 'adapter',\n",
       " 'adapting',\n",
       " 'adaption',\n",
       " 'adaptive',\n",
       " 'adaptiveness',\n",
       " 'adapts',\n",
       " 'aday',\n",
       " 'add',\n",
       " 'adda',\n",
       " 'addable',\n",
       " 'addax',\n",
       " 'adde',\n",
       " 'added',\n",
       " 'addendum',\n",
       " 'adder',\n",
       " 'addeth',\n",
       " 'addicks',\n",
       " 'addict',\n",
       " 'addicted',\n",
       " 'addicti',\n",
       " 'addicting',\n",
       " 'addiction',\n",
       " 'addictive',\n",
       " 'addictively',\n",
       " 'addie',\n",
       " 'addin',\n",
       " 'adding',\n",
       " 'addington',\n",
       " 'addingtonian',\n",
       " 'addingtonians',\n",
       " 'addingtons',\n",
       " 'addio',\n",
       " 'addios',\n",
       " 'addiscombe',\n",
       " 'addison',\n",
       " 'addisonian',\n",
       " 'addisons',\n",
       " 'addition',\n",
       " 'additional',\n",
       " 'additionally',\n",
       " 'addix',\n",
       " 'addle',\n",
       " 'addled',\n",
       " 'addlehead',\n",
       " 'addlepated',\n",
       " 'addlepates',\n",
       " 'addles',\n",
       " 'addleton',\n",
       " 'addling',\n",
       " 'address',\n",
       " 'addressed',\n",
       " 'addressee',\n",
       " 'addresser',\n",
       " 'addressing',\n",
       " 'addrionick',\n",
       " 'adduce',\n",
       " 'adduced',\n",
       " 'adducing',\n",
       " 'addwess',\n",
       " 'adeea',\n",
       " 'adegah',\n",
       " 'adel',\n",
       " 'adela',\n",
       " 'adelaide',\n",
       " 'adelantado',\n",
       " 'adelante',\n",
       " 'adele',\n",
       " 'adeles',\n",
       " 'adelina',\n",
       " 'adeline',\n",
       " 'adeliza',\n",
       " 'adelle',\n",
       " 'adelles',\n",
       " 'adelphi',\n",
       " 'ademptum',\n",
       " 'aden',\n",
       " 'adeney',\n",
       " 'adenoid',\n",
       " 'adenoidal',\n",
       " 'adeo',\n",
       " 'adeos',\n",
       " 'adept',\n",
       " 'adeptship',\n",
       " 'adequacy',\n",
       " 'adequate',\n",
       " 'adequately',\n",
       " 'ader',\n",
       " 'adesso',\n",
       " 'adeste',\n",
       " 'adesto',\n",
       " 'adharc',\n",
       " 'adhem',\n",
       " 'adhere',\n",
       " 'adhered',\n",
       " 'adherence',\n",
       " 'adherent',\n",
       " 'adherer',\n",
       " 'adheres',\n",
       " 'adhering',\n",
       " 'adhesion',\n",
       " 'adhesive',\n",
       " 'adhesively',\n",
       " 'adhesiveness',\n",
       " 'adhmiral',\n",
       " 'adhémar',\n",
       " 'adianthum',\n",
       " 'adienx',\n",
       " 'adieu',\n",
       " 'adige',\n",
       " 'adigrat',\n",
       " ...]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels=y_train\n",
    "test_labels=y_test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "del train['clean_data']\n",
    "del test['clean_data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 1, 'kernel': 'linear'}"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc = svm.SVC(gamma='auto')\n",
    "parameters = {'kernel':('linear', 'rbf'), 'C':[1, 10]}\n",
    "\n",
    "clf = GridSearchCV(svc, parameters)\n",
    "\n",
    "clf.fit(train,train_labels)\n",
    "\n",
    "clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy: 1.0 \n",
      "test accuracy: 0.8461538461538461 \n"
     ]
    }
   ],
   "source": [
    "predictions=clf.predict(test)\n",
    "train_pred=clf.predict(train)\n",
    "\n",
    "print(\"train accuracy: {} \".format(accuracy_score(train_labels,train_pred,normalize=True, sample_weight=None)))\n",
    "print(\"test accuracy: {} \".format(accuracy_score(test_labels,predictions,normalize=True, sample_weight=None))) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:     \n",
      "[[  0   0   0   0   0   1   0   0   0]\n",
      " [  0   0   0   0   0   2   0   0   0]\n",
      " [  0   0  11   0   0  22   0   0   0]\n",
      " [  0   0   0   0   0   2   0   0   0]\n",
      " [  0   0   0   0   0   2   0   0   0]\n",
      " [  0   0   0   0   0 238   0   0   0]\n",
      " [  0   0   0   0   0   5   0   0   0]\n",
      " [  0   0   0   0   0   8   0   2   1]\n",
      " [  0   0   0   0   0   3   0   0   2]]\n",
      "\n",
      "\n",
      "Classification Report:       \n",
      "                             precision    recall  f1-score   support\n",
      "\n",
      "                 Allegories       0.00      0.00      0.00         1\n",
      "          Christmas Stories       0.00      0.00      0.00         2\n",
      "      Detective and Mystery       1.00      0.33      0.50        33\n",
      "           Ghost and Horror       0.00      0.00      0.00         2\n",
      "Humorous and Wit and Satire       0.00      0.00      0.00         2\n",
      "                   Literary       0.84      1.00      0.91       238\n",
      "           Love and Romance       0.00      0.00      0.00         5\n",
      "          Sea and Adventure       1.00      0.18      0.31        11\n",
      "            Western Stories       0.67      0.40      0.50         5\n",
      "\n",
      "                   accuracy                           0.85       299\n",
      "                  macro avg       0.39      0.21      0.25       299\n",
      "               weighted avg       0.83      0.85      0.80       299\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Confusion Matrix:     \")\n",
    "print(confusion_matrix(test_labels, predictions))\n",
    "        \n",
    "print(\"\\n\\nClassification Report:       \")\n",
    "print(classification_report(test_labels, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 0.001, 'gamma': 0.001}"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_grid1 = {'C': [0.001, 0.01, 0.1, 1, 10], 'gamma' : [1e-3, 1e-4]}\n",
    "grid_search1 = GridSearchCV(svm.SVC(kernel='rbf',gamma='auto'), param_grid1)\n",
    "grid_search1.fit(train,train_labels)\n",
    "grid_search1.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions=grid_search1.predict(test)\n",
    "train_pred=grid_search1.predict(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy: 0.7971223021582734 \n",
      "test accuracy: 0.7959866220735786 \n"
     ]
    }
   ],
   "source": [
    "print(\"train accuracy: {} \".format(accuracy_score(train_labels,train_pred,normalize=True, sample_weight=None)))\n",
    "print(\"test accuracy: {} \".format(accuracy_score(test_labels,predictions,normalize=True, sample_weight=None))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:     \n",
      "[[  0   0   0   0   0   1   0   0   0]\n",
      " [  0   0   0   0   0   2   0   0   0]\n",
      " [  0   0   0   0   0  33   0   0   0]\n",
      " [  0   0   0   0   0   2   0   0   0]\n",
      " [  0   0   0   0   0   2   0   0   0]\n",
      " [  0   0   0   0   0 238   0   0   0]\n",
      " [  0   0   0   0   0   5   0   0   0]\n",
      " [  0   0   0   0   0  11   0   0   0]\n",
      " [  0   0   0   0   0   5   0   0   0]]\n",
      "\n",
      "\n",
      "Classification Report:       \n",
      "                             precision    recall  f1-score   support\n",
      "\n",
      "                 Allegories       0.00      0.00      0.00         1\n",
      "          Christmas Stories       0.00      0.00      0.00         2\n",
      "      Detective and Mystery       0.00      0.00      0.00        33\n",
      "           Ghost and Horror       0.00      0.00      0.00         2\n",
      "Humorous and Wit and Satire       0.00      0.00      0.00         2\n",
      "                   Literary       0.80      1.00      0.89       238\n",
      "           Love and Romance       0.00      0.00      0.00         5\n",
      "          Sea and Adventure       0.00      0.00      0.00        11\n",
      "            Western Stories       0.00      0.00      0.00         5\n",
      "\n",
      "                   accuracy                           0.80       299\n",
      "                  macro avg       0.09      0.11      0.10       299\n",
      "               weighted avg       0.63      0.80      0.71       299\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "print(\"Confusion Matrix:     \")\n",
    "print(confusion_matrix(test_labels, predictions))\n",
    "        \n",
    "print(\"\\n\\nClassification Report:       \")\n",
    "print(classification_report(test_labels, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 10, 'gamma': 0.1}"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_grid = {'C': [0.001, 0.01, 0.1, 1, 10], 'gamma' : [0.001, 0.01, 0.1, 1]}\n",
    "grid_search = GridSearchCV(svm.SVC(kernel='rbf',gamma='auto'), param_grid)\n",
    "grid_search.fit(train,train_labels)\n",
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions=grid_search.predict(test)\n",
    "train_pred=grid_search.predict(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy: 1.0 \n",
      "test accuracy: 0.8394648829431438 \n"
     ]
    }
   ],
   "source": [
    "print(\"train accuracy: {} \".format(accuracy_score(train_labels,train_pred,normalize=True, sample_weight=None)))\n",
    "print(\"test accuracy: {} \".format(accuracy_score(test_labels,predictions,normalize=True, sample_weight=None))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:     \n",
      "[[  0   0   0   0   0   1   0   0   0]\n",
      " [  0   0   0   0   0   2   0   0   0]\n",
      " [  0   0  10   0   0  23   0   0   0]\n",
      " [  0   0   0   0   0   2   0   0   0]\n",
      " [  0   0   0   0   0   2   0   0   0]\n",
      " [  0   0   0   0   0 238   0   0   0]\n",
      " [  0   0   0   0   0   5   0   0   0]\n",
      " [  0   0   0   0   0   8   0   2   1]\n",
      " [  0   0   0   0   0   4   0   0   1]]\n",
      "\n",
      "\n",
      "Classification Report:       \n",
      "                             precision    recall  f1-score   support\n",
      "\n",
      "                 Allegories       0.00      0.00      0.00         1\n",
      "          Christmas Stories       0.00      0.00      0.00         2\n",
      "      Detective and Mystery       1.00      0.30      0.47        33\n",
      "           Ghost and Horror       0.00      0.00      0.00         2\n",
      "Humorous and Wit and Satire       0.00      0.00      0.00         2\n",
      "                   Literary       0.84      1.00      0.91       238\n",
      "           Love and Romance       0.00      0.00      0.00         5\n",
      "          Sea and Adventure       1.00      0.18      0.31        11\n",
      "            Western Stories       0.50      0.20      0.29         5\n",
      "\n",
      "                   accuracy                           0.84       299\n",
      "                  macro avg       0.37      0.19      0.22       299\n",
      "               weighted avg       0.82      0.84      0.79       299\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "print(\"Confusion Matrix:     \")\n",
    "print(confusion_matrix(test_labels, predictions))\n",
    "        \n",
    "print(\"\\n\\nClassification Report:       \")\n",
    "print(classification_report(test_labels, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 1, 'gamma': 0.001}"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_grid = {'C': [0.001, 0.01, 0.1, 1, 10], 'gamma' : [0.001, 0.01, 0.1, 1, 10, 100, 1000]}\n",
    "grid_search = GridSearchCV(svm.SVC(kernel='linear',gamma='auto'), param_grid)\n",
    "grid_search.fit(train,train_labels)\n",
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions=grid_search.predict(test)\n",
    "train_pred=grid_search.predict(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy: 1.0 \n",
      "test accuracy: 0.8461538461538461 \n"
     ]
    }
   ],
   "source": [
    "print(\"train accuracy: {} \".format(accuracy_score(train_labels,train_pred,normalize=True, sample_weight=None)))\n",
    "print(\"test accuracy: {} \".format(accuracy_score(test_labels,predictions,normalize=True, sample_weight=None))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:     \n",
      "[[  0   0   0   0   0   1   0   0   0]\n",
      " [  0   0   0   0   0   2   0   0   0]\n",
      " [  0   0  11   0   0  22   0   0   0]\n",
      " [  0   0   0   0   0   2   0   0   0]\n",
      " [  0   0   0   0   0   2   0   0   0]\n",
      " [  0   0   0   0   0 238   0   0   0]\n",
      " [  0   0   0   0   0   5   0   0   0]\n",
      " [  0   0   0   0   0   8   0   2   1]\n",
      " [  0   0   0   0   0   3   0   0   2]]\n",
      "\n",
      "\n",
      "Classification Report:       \n",
      "                             precision    recall  f1-score   support\n",
      "\n",
      "                 Allegories       0.00      0.00      0.00         1\n",
      "          Christmas Stories       0.00      0.00      0.00         2\n",
      "      Detective and Mystery       1.00      0.33      0.50        33\n",
      "           Ghost and Horror       0.00      0.00      0.00         2\n",
      "Humorous and Wit and Satire       0.00      0.00      0.00         2\n",
      "                   Literary       0.84      1.00      0.91       238\n",
      "           Love and Romance       0.00      0.00      0.00         5\n",
      "          Sea and Adventure       1.00      0.18      0.31        11\n",
      "            Western Stories       0.67      0.40      0.50         5\n",
      "\n",
      "                   accuracy                           0.85       299\n",
      "                  macro avg       0.39      0.21      0.25       299\n",
      "               weighted avg       0.83      0.85      0.80       299\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "print(\"Confusion Matrix:     \")\n",
    "print(confusion_matrix(test_labels, predictions))\n",
    "        \n",
    "print(\"\\n\\nClassification Report:       \")\n",
    "print(classification_report(test_labels, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnb = MultinomialNB() \n",
    "model1 = mnb.fit(train, train_labels)\n",
    "predictions = mnb.predict(test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy: 0.7971223021582734 \n",
      "test accuracy: 0.7959866220735786 \n"
     ]
    }
   ],
   "source": [
    "print(\"train accuracy: {} \".format(mnb.fit(train, train_labels).score(train, train_labels)))\n",
    "print(\"test accuracy: {} \".format(mnb.fit(train, train_labels).score(test, test_labels))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:     \n",
      "[[  0   0   0   0   0   1   0   0   0]\n",
      " [  0   0   0   0   0   2   0   0   0]\n",
      " [  0   0   0   0   0  33   0   0   0]\n",
      " [  0   0   0   0   0   2   0   0   0]\n",
      " [  0   0   0   0   0   2   0   0   0]\n",
      " [  0   0   0   0   0 238   0   0   0]\n",
      " [  0   0   0   0   0   5   0   0   0]\n",
      " [  0   0   0   0   0  11   0   0   0]\n",
      " [  0   0   0   0   0   5   0   0   0]]\n",
      "\n",
      "\n",
      "Classification Report:       \n",
      "                             precision    recall  f1-score   support\n",
      "\n",
      "                 Allegories       0.00      0.00      0.00         1\n",
      "          Christmas Stories       0.00      0.00      0.00         2\n",
      "      Detective and Mystery       0.00      0.00      0.00        33\n",
      "           Ghost and Horror       0.00      0.00      0.00         2\n",
      "Humorous and Wit and Satire       0.00      0.00      0.00         2\n",
      "                   Literary       0.80      1.00      0.89       238\n",
      "           Love and Romance       0.00      0.00      0.00         5\n",
      "          Sea and Adventure       0.00      0.00      0.00        11\n",
      "            Western Stories       0.00      0.00      0.00         5\n",
      "\n",
      "                   accuracy                           0.80       299\n",
      "                  macro avg       0.09      0.11      0.10       299\n",
      "               weighted avg       0.63      0.80      0.71       299\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "print(\"Confusion Matrix:     \")\n",
    "print(confusion_matrix(test_labels, predictions))\n",
    "        \n",
    "print(\"\\n\\nClassification Report:       \")\n",
    "print(classification_report(test_labels, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnb = GaussianNB() \n",
    "model1 = gnb.fit(train, train_labels)\n",
    "predictions = gnb.predict(test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy: 1.0 \n",
      "test accuracy: 0.8193979933110368 \n"
     ]
    }
   ],
   "source": [
    "print(\"train accuracy: {} \".format(gnb.fit(train, train_labels).score(train, train_labels)))\n",
    "print(\"test accuracy: {} \".format(gnb.fit(train, train_labels).score(test, test_labels))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:     \n",
      "[[  0   0   0   0   0   1   0   0   0]\n",
      " [  0   0   1   0   0   1   0   0   0]\n",
      " [  0   0  10   0   0  23   0   0   0]\n",
      " [  0   0   0   0   0   2   0   0   0]\n",
      " [  0   0   0   0   0   2   0   0   0]\n",
      " [  0   0   2   0   0 232   1   3   0]\n",
      " [  0   0   0   0   0   5   0   0   0]\n",
      " [  0   0   0   0   0   9   0   2   0]\n",
      " [  0   0   0   0   0   4   0   0   1]]\n",
      "\n",
      "\n",
      "Classification Report:       \n",
      "                             precision    recall  f1-score   support\n",
      "\n",
      "                 Allegories       0.00      0.00      0.00         1\n",
      "          Christmas Stories       0.00      0.00      0.00         2\n",
      "      Detective and Mystery       0.77      0.30      0.43        33\n",
      "           Ghost and Horror       0.00      0.00      0.00         2\n",
      "Humorous and Wit and Satire       0.00      0.00      0.00         2\n",
      "                   Literary       0.83      0.97      0.90       238\n",
      "           Love and Romance       0.00      0.00      0.00         5\n",
      "          Sea and Adventure       0.40      0.18      0.25        11\n",
      "            Western Stories       1.00      0.20      0.33         5\n",
      "\n",
      "                   accuracy                           0.82       299\n",
      "                  macro avg       0.33      0.18      0.21       299\n",
      "               weighted avg       0.78      0.82      0.78       299\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "print(\"Confusion Matrix:     \")\n",
    "print(confusion_matrix(test_labels, predictions))\n",
    "        \n",
    "print(\"\\n\\nClassification Report:       \")\n",
    "print(classification_report(test_labels, predictions))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
